{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec93c4d",
   "metadata": {},
   "source": [
    "## Tối ưu hàm mục tiêu:\n",
    "$$\n",
    "\\boxed{\n",
    "\\min_{G} E_{z \\sim p_z} [\\mathcal{L}_{\\text{GAN}}(G(z)) + \\lambda \\cdot Q(G(z))]\n",
    "}\n",
    "$$\n",
    "Trong đó:\n",
    "* $G(z)$: ảnh được sinh từ latent vector $z$\n",
    "* $Q(G(z))$: hàm đánh giá **chất lượng ảnh sinh**\n",
    "* $\\lambda$: hệ số điều chỉnh mức phạt của loss chất lượng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f1c9b",
   "metadata": {},
   "source": [
    "### Thay vì chỉ dùng $X$ là độ nhiễu và $Y$ là mức độ thật (mà không rõ đo thế nào), ta định nghĩa lại như sau:\n",
    "\n",
    "**a. Feature distance $D_r$ – Độ khác biệt với ảnh thật:**\n",
    "\n",
    "* Cho ảnh sinh $I_{gen}$, và ảnh thật tương ứng $I_{real}$\n",
    "* Ta tính:\n",
    "$$\n",
    "D_r = \\| \\phi(I_{gen}) - \\phi(I_{real}) \\|_2\n",
    "$$\n",
    "\n",
    "Trong đó $\\phi(\\cdot)$ là feature extractor (VD: tầng giữa của VGG16 hoặc ResNet)\n",
    "\n",
    "$\\to$ **$D_r$ càng nhỏ thì ảnh càng giống thật**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee506e0",
   "metadata": {},
   "source": [
    "**b. Noise-level estimator $N_g$ – Mức độ nhiễu nội tại:**\n",
    "\n",
    "* Dựa trên thống kê gradient hoặc Laplacian:\n",
    "$$\n",
    "N_g = \\text{Var}(\\nabla I_{gen}) \\quad (\\text{hoặc}) \\quad \\text{Laplacian-based energy}\n",
    "$$\n",
    "\n",
    "$\\to$ **Càng nhỏ thì ảnh càng mượt, ít nhiễu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4487d",
   "metadata": {},
   "source": [
    "## Hàm đánh giá chất lượng tổng hợp\n",
    "$$\n",
    "Q(I_{gen}) = \\alpha \\cdot \\text{soft}(N_g) + \\beta \\cdot \\text{soft}(D_r)\n",
    "$$\n",
    "\n",
    "* $\\text{soft}(\\cdot)$ là hàm chuẩn hóa tuyến tính về khoảng [0,1]\n",
    "* $\\alpha, \\beta$: trọng số học được hoặc chọn dựa vào yêu cầu (ví dụ: penalize nhiễu mạnh hơn)\n",
    "\n",
    "$\\to$ **Càng nhỏ, ảnh càng tốt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4805",
   "metadata": {},
   "source": [
    "## Điều kiện đánh giá hoặc regularizer cho GAN\n",
    "**a. Ngưỡng hóa (Thresholding):**\n",
    "$$\n",
    "Q(I_{gen}) < \\tau\n",
    "$$\n",
    "$\\to$ giống như cách bạn đề xuất ban đầu, nhưng với các thành phần có định nghĩa rõ ràng hơn.\n",
    "\n",
    "**b. Dùng làm Regularizer trong loss GAN:**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{GAN}} + \\lambda \\cdot Q(I_{gen})\n",
    "$$\n",
    "$\\to$ ép Generator sinh ảnh có **nhiễu thấp** và **giống thật** trong feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd0236",
   "metadata": {},
   "source": [
    "## Mô hình tổng quát\n",
    "$$\n",
    "\\boxed{\n",
    "Q(I_{gen}) = \\alpha \\cdot \\text{Norm}(\\text{NoiseLevel}(I_{gen})) + \\beta \\cdot \\text{Norm}(\\| \\phi(I_{gen}) - \\phi(I_{real}) \\|)\n",
    "}\n",
    "$$\n",
    "\n",
    "* **Ngưỡng đánh giá:**\n",
    "    $$\n",
    "    Q(I_{gen}) < \\tau\n",
    "    $$\n",
    "\n",
    "* **Hoặc dùng trong loss:**\n",
    "    $$\n",
    "    \\min_{G} E_{z \\sim p_z} [\\mathcal{L}_{\\text{GAN}}(G(z)) + \\lambda \\cdot Q(G(z))]\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd18f49",
   "metadata": {},
   "source": [
    "**Hàm chuẩn hóa:**\n",
    "$$\n",
    "\\text{Norm}(x) = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "$\\to$ Đưa cả hai thành phần về cùng thang đo [0,1]\n",
    "\n",
    "\n",
    "**Vai trò các hệ số**\n",
    "$$\n",
    "\\alpha + \\beta = 1 \\quad (\\text{nếu cần chuẩn hóa trọng số})\n",
    "$$\n",
    "* $\\alpha$: trọng số cho độ nhiễu $\\to$ kiểm soát độ sắc nét\n",
    "* $\\beta$: trọng số cho độ giống thật $\\to$ kiểm soát tính chân thực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806ca6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "def show_generated_images(generator, extractor, epoch, alpha=0.5, beta=0.5, device='cpu'):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(16, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "        \n",
    "        # Lấy mẫu ảnh thật để so sánh\n",
    "        real_batch = next(iter(dataloader))[0][:16].to(device)\n",
    "        q_score = compute_quality(fake_images, real_batch, extractor, alpha, beta)\n",
    "\n",
    "        grid = vutils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Generated Images - Epoch {epoch+1}\\nQ(I) Score = {q_score.item():.4f}\")\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()\n",
    "    generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12571c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === Hyperparameters ===\n",
    "latent_dim = 100\n",
    "batch_size = 64\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "lambda_q = 0.1  # đã giảm\n",
    "epochs = 500\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === MNIST Dataset ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# === Generator ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z).view(-1, 1, 28, 28)\n",
    "\n",
    "# === Discriminator ===\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Feature Extractor ===\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(*list(D.net.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c170fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hàm tính noise & khoảng cách đặc trưng ===\n",
    "def compute_noise_level(img):\n",
    "    grad_x = img[:, :, :, 1:] - img[:, :, :, :-1]\n",
    "    grad_y = img[:, :, 1:, :] - img[:, :, :-1, :]\n",
    "    return (grad_x.abs().mean() + grad_y.abs().mean())\n",
    "\n",
    "def compute_feature_distance(gen_img, real_img, extractor):\n",
    "    feat_gen = extractor(gen_img)\n",
    "    feat_real = extractor(real_img)\n",
    "    return F.mse_loss(feat_gen, feat_real)\n",
    "\n",
    "# === Chuẩn hóa động Q(I) ===\n",
    "Q_MIN, Q_MAX = 1e10, -1e10\n",
    "\n",
    "def compute_quality(gen_img, real_img, extractor, alpha=0.5, beta=0.5):\n",
    "    global Q_MIN, Q_MAX\n",
    "    noise = compute_noise_level(gen_img)\n",
    "    dist = compute_feature_distance(gen_img, real_img, extractor)\n",
    "    raw_q = alpha * noise + beta * dist\n",
    "\n",
    "    Q_MIN = min(Q_MIN, raw_q.item())\n",
    "    Q_MAX = max(Q_MAX, raw_q.item())\n",
    "\n",
    "    return (raw_q - Q_MIN) / (Q_MAX - Q_MIN + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0fd0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_generated_images(generator, extractor, epoch, device='cpu'):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(16, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "\n",
    "        real_batch = next(iter(dataloader))[0][:16].to(device)\n",
    "        q_score = compute_quality(fake_images, real_batch, extractor, alpha, beta)\n",
    "\n",
    "        grid = vutils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch+1} | Q(I): {q_score.item():.4f}\")\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.savefig(f\"gen_images/epoch_{epoch+1:03}_Q{q_score.item():.2f}.png\")\n",
    "        plt.close()\n",
    "    generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56c42eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: G_loss=8.6350, D_loss=0.0003, Q=0.9504\n",
      "Epoch 2: G_loss=9.3719, D_loss=0.0001, Q=0.9838\n",
      "Epoch 3: G_loss=8.9603, D_loss=0.0003, Q=0.9961\n",
      "Epoch 4: G_loss=6.2807, D_loss=0.0087, Q=0.7480\n",
      "Epoch 5: G_loss=8.6264, D_loss=0.0008, Q=0.4410\n",
      "Epoch 6: G_loss=9.6516, D_loss=0.0001, Q=0.3505\n",
      "Epoch 7: G_loss=26.5741, D_loss=0.0057, Q=0.2197\n",
      "Epoch 8: G_loss=7.3377, D_loss=0.0051, Q=0.0874\n",
      "Epoch 9: G_loss=6.7884, D_loss=0.0182, Q=0.0672\n",
      "Epoch 10: G_loss=6.8156, D_loss=0.0090, Q=0.0737\n",
      "Epoch 11: G_loss=5.2756, D_loss=0.1711, Q=0.0666\n",
      "Epoch 12: G_loss=7.1442, D_loss=0.0718, Q=0.0461\n",
      "Epoch 13: G_loss=6.2465, D_loss=0.0259, Q=0.0563\n",
      "Epoch 14: G_loss=5.3260, D_loss=0.0429, Q=0.0739\n",
      "Epoch 15: G_loss=5.9440, D_loss=0.1859, Q=0.0562\n",
      "Epoch 16: G_loss=6.5515, D_loss=0.2095, Q=0.0646\n",
      "Epoch 17: G_loss=4.2887, D_loss=0.3731, Q=0.0589\n",
      "Epoch 18: G_loss=5.6949, D_loss=0.6275, Q=0.0998\n",
      "Epoch 19: G_loss=4.7904, D_loss=0.6765, Q=0.1060\n",
      "Epoch 20: G_loss=3.2054, D_loss=1.2666, Q=0.1337\n",
      "Epoch 21: G_loss=4.8611, D_loss=0.3647, Q=0.2002\n",
      "Epoch 22: G_loss=3.8787, D_loss=0.9588, Q=0.1998\n",
      "Epoch 23: G_loss=3.0050, D_loss=0.7037, Q=0.2876\n",
      "Epoch 24: G_loss=3.2936, D_loss=2.1876, Q=0.3204\n",
      "Epoch 25: G_loss=2.0531, D_loss=2.2676, Q=0.2686\n",
      "Epoch 26: G_loss=1.3005, D_loss=2.0213, Q=0.3726\n",
      "Epoch 27: G_loss=3.7129, D_loss=1.1310, Q=0.3246\n",
      "Epoch 28: G_loss=3.6103, D_loss=2.6357, Q=0.3001\n",
      "Epoch 29: G_loss=2.1259, D_loss=0.9994, Q=0.3800\n",
      "Epoch 30: G_loss=2.2169, D_loss=0.7771, Q=0.3732\n",
      "Epoch 31: G_loss=3.2857, D_loss=1.0913, Q=0.3231\n",
      "Epoch 32: G_loss=2.4413, D_loss=0.9942, Q=0.4098\n",
      "Epoch 33: G_loss=2.7366, D_loss=1.8567, Q=0.3974\n",
      "Epoch 34: G_loss=1.1570, D_loss=1.1300, Q=0.4353\n",
      "Epoch 35: G_loss=2.1553, D_loss=1.9903, Q=0.3601\n",
      "Epoch 36: G_loss=2.2056, D_loss=1.3986, Q=0.5042\n",
      "Epoch 37: G_loss=1.7754, D_loss=1.3865, Q=0.4280\n",
      "Epoch 38: G_loss=1.7115, D_loss=0.9934, Q=0.4158\n",
      "Epoch 39: G_loss=2.5221, D_loss=1.4953, Q=0.5593\n",
      "Epoch 40: G_loss=2.1842, D_loss=1.3601, Q=0.5111\n",
      "Epoch 41: G_loss=2.0516, D_loss=0.7445, Q=0.4680\n",
      "Epoch 42: G_loss=1.7572, D_loss=1.2020, Q=0.5090\n",
      "Epoch 43: G_loss=1.3325, D_loss=2.1130, Q=0.5372\n",
      "Epoch 44: G_loss=1.4661, D_loss=1.3128, Q=0.5991\n",
      "Epoch 45: G_loss=2.6502, D_loss=1.0661, Q=0.7281\n",
      "Epoch 46: G_loss=1.1698, D_loss=1.3834, Q=0.6056\n",
      "Epoch 47: G_loss=2.0414, D_loss=1.0407, Q=0.6363\n",
      "Epoch 48: G_loss=1.3634, D_loss=1.4117, Q=0.6640\n",
      "Epoch 49: G_loss=1.5263, D_loss=1.2616, Q=0.6632\n",
      "Epoch 50: G_loss=1.5703, D_loss=1.1849, Q=0.6683\n",
      "Epoch 51: G_loss=1.4847, D_loss=1.1859, Q=0.6710\n",
      "Epoch 52: G_loss=1.0884, D_loss=0.8187, Q=0.8900\n",
      "Epoch 53: G_loss=1.8486, D_loss=1.1342, Q=0.7428\n",
      "Epoch 54: G_loss=1.2681, D_loss=0.9096, Q=0.6919\n",
      "Epoch 55: G_loss=1.7564, D_loss=1.1200, Q=0.7511\n",
      "Epoch 56: G_loss=1.8212, D_loss=1.0437, Q=0.8785\n",
      "Epoch 57: G_loss=1.5866, D_loss=0.8253, Q=0.8506\n",
      "Epoch 58: G_loss=1.7543, D_loss=1.4832, Q=0.8394\n",
      "Epoch 59: G_loss=1.5211, D_loss=0.9806, Q=0.8701\n",
      "Epoch 60: G_loss=1.9075, D_loss=1.5134, Q=0.7382\n",
      "Epoch 61: G_loss=1.8361, D_loss=0.9078, Q=0.8577\n",
      "Epoch 62: G_loss=1.5265, D_loss=1.3459, Q=0.8374\n",
      "Epoch 63: G_loss=1.4128, D_loss=1.1339, Q=0.9085\n",
      "Epoch 64: G_loss=1.5095, D_loss=1.3229, Q=0.7876\n",
      "Epoch 65: G_loss=1.6972, D_loss=1.2772, Q=0.7966\n",
      "Epoch 66: G_loss=1.6839, D_loss=1.2563, Q=0.8787\n",
      "Epoch 67: G_loss=1.3901, D_loss=0.8820, Q=0.8445\n",
      "Epoch 68: G_loss=1.5287, D_loss=0.9971, Q=0.7334\n",
      "Epoch 69: G_loss=1.7282, D_loss=1.0308, Q=0.8067\n",
      "Epoch 70: G_loss=2.1035, D_loss=0.9957, Q=0.9344\n",
      "Epoch 71: G_loss=2.1862, D_loss=0.8381, Q=0.8301\n",
      "Epoch 72: G_loss=2.1397, D_loss=0.9186, Q=0.8282\n",
      "Epoch 73: G_loss=1.3264, D_loss=0.9083, Q=0.8880\n",
      "Epoch 74: G_loss=1.4242, D_loss=1.0163, Q=0.8780\n",
      "Epoch 75: G_loss=1.5500, D_loss=1.0199, Q=0.8600\n",
      "Epoch 76: G_loss=1.4522, D_loss=0.9346, Q=0.9029\n",
      "Epoch 77: G_loss=1.6187, D_loss=0.9528, Q=0.8297\n",
      "Epoch 78: G_loss=1.8525, D_loss=0.8623, Q=0.8452\n",
      "Epoch 79: G_loss=0.9612, D_loss=1.0498, Q=0.8352\n",
      "Epoch 80: G_loss=0.9362, D_loss=1.2331, Q=0.8709\n",
      "Epoch 81: G_loss=1.3417, D_loss=1.1801, Q=0.7339\n",
      "Epoch 82: G_loss=1.3278, D_loss=0.9645, Q=0.7362\n",
      "Epoch 83: G_loss=1.3063, D_loss=0.8073, Q=0.8050\n",
      "Epoch 84: G_loss=1.3897, D_loss=1.1933, Q=0.8451\n",
      "Epoch 85: G_loss=1.5594, D_loss=1.0044, Q=0.8081\n",
      "Epoch 86: G_loss=1.3161, D_loss=1.1442, Q=0.8055\n",
      "Epoch 87: G_loss=1.2171, D_loss=1.1445, Q=0.9012\n",
      "Epoch 88: G_loss=1.3276, D_loss=1.1857, Q=0.8392\n",
      "Epoch 89: G_loss=1.2890, D_loss=1.1147, Q=0.9437\n",
      "Epoch 90: G_loss=0.9914, D_loss=1.4764, Q=0.8091\n",
      "Epoch 91: G_loss=0.9538, D_loss=1.2559, Q=0.7781\n",
      "Epoch 92: G_loss=1.7585, D_loss=0.9463, Q=0.8556\n",
      "Epoch 93: G_loss=1.3006, D_loss=1.0492, Q=0.8314\n",
      "Epoch 94: G_loss=1.3308, D_loss=0.7844, Q=0.9045\n",
      "Epoch 95: G_loss=1.6811, D_loss=1.1399, Q=0.8791\n",
      "Epoch 96: G_loss=1.9470, D_loss=0.9162, Q=0.9018\n",
      "Epoch 97: G_loss=1.5128, D_loss=1.0815, Q=0.7538\n",
      "Epoch 98: G_loss=1.3194, D_loss=1.0516, Q=0.8654\n",
      "Epoch 99: G_loss=1.7139, D_loss=0.9756, Q=0.8309\n",
      "Epoch 100: G_loss=1.9314, D_loss=1.0135, Q=0.8685\n",
      "Epoch 101: G_loss=1.7057, D_loss=1.2016, Q=0.8921\n",
      "Epoch 102: G_loss=1.7662, D_loss=1.2099, Q=0.8730\n",
      "Epoch 103: G_loss=1.1819, D_loss=0.9365, Q=0.8646\n",
      "Epoch 104: G_loss=1.8237, D_loss=1.1895, Q=0.9072\n",
      "Epoch 105: G_loss=1.6796, D_loss=0.8934, Q=0.7682\n",
      "Epoch 106: G_loss=1.2010, D_loss=1.0235, Q=0.9016\n",
      "Epoch 107: G_loss=1.1912, D_loss=0.9777, Q=0.8077\n",
      "Epoch 108: G_loss=1.6687, D_loss=0.9690, Q=0.8124\n",
      "Epoch 109: G_loss=1.9453, D_loss=0.8054, Q=0.9097\n",
      "Epoch 110: G_loss=1.7306, D_loss=0.8078, Q=0.8908\n",
      "Epoch 111: G_loss=1.5418, D_loss=0.9767, Q=0.8172\n",
      "Epoch 112: G_loss=1.4144, D_loss=1.0657, Q=0.7851\n",
      "Epoch 113: G_loss=1.7336, D_loss=1.2350, Q=0.8841\n",
      "Epoch 114: G_loss=1.5080, D_loss=0.8471, Q=0.9099\n",
      "Epoch 115: G_loss=1.8380, D_loss=0.7969, Q=0.9236\n",
      "Epoch 116: G_loss=1.6222, D_loss=0.8736, Q=0.7865\n",
      "Epoch 117: G_loss=1.5510, D_loss=1.3827, Q=0.7879\n",
      "Epoch 118: G_loss=1.5533, D_loss=1.2441, Q=0.9077\n",
      "Epoch 119: G_loss=1.1174, D_loss=1.7705, Q=0.8071\n",
      "Epoch 120: G_loss=1.3963, D_loss=0.9383, Q=0.8185\n",
      "Epoch 121: G_loss=1.7806, D_loss=1.0502, Q=0.8443\n",
      "Epoch 122: G_loss=1.3124, D_loss=1.1344, Q=0.9680\n",
      "Epoch 123: G_loss=1.2032, D_loss=1.0659, Q=0.9018\n",
      "Epoch 124: G_loss=1.4160, D_loss=1.1240, Q=0.8263\n",
      "Epoch 125: G_loss=1.6418, D_loss=0.9419, Q=0.8591\n",
      "Epoch 126: G_loss=1.2790, D_loss=0.8883, Q=0.8362\n",
      "Epoch 127: G_loss=1.1829, D_loss=1.0223, Q=0.7786\n",
      "Epoch 128: G_loss=1.4294, D_loss=0.9494, Q=0.7822\n",
      "Epoch 129: G_loss=1.1540, D_loss=1.0733, Q=0.8008\n",
      "Epoch 130: G_loss=1.4824, D_loss=1.6023, Q=0.7600\n",
      "Epoch 131: G_loss=2.0461, D_loss=0.8436, Q=0.7699\n",
      "Epoch 132: G_loss=2.0409, D_loss=1.0012, Q=0.8178\n",
      "Epoch 133: G_loss=1.4368, D_loss=1.0134, Q=0.9191\n",
      "Epoch 134: G_loss=1.3542, D_loss=0.9649, Q=0.8770\n",
      "Epoch 135: G_loss=1.3223, D_loss=0.8771, Q=0.8512\n",
      "Epoch 136: G_loss=1.4124, D_loss=1.2684, Q=0.7889\n",
      "Epoch 137: G_loss=1.3087, D_loss=1.2087, Q=0.7615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# === Huấn luyện ===\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# === Train Discriminator ===\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = _Image_fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Khởi tạo mô hình ===\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "feature_extractor = FeatureExtractor(D).to(device)\n",
    "\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=1e-3)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=5e-4)  # giảm LR của D\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# === Tạo thư mục lưu ảnh\n",
    "os.makedirs(\"gen_images\", exist_ok=True)\n",
    "\n",
    "G_losses, D_losses, Q_scores = [], [], []\n",
    "\n",
    "# === Huấn luyện ===\n",
    "for epoch in range(epochs):\n",
    "    for real_imgs, _ in dataloader:\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "        fake_imgs = G(z).detach()\n",
    "        D_real = D(real_imgs)\n",
    "        D_fake = D(fake_imgs)\n",
    "\n",
    "        loss_D = loss_fn(D_real, torch.ones_like(D_real)) + \\\n",
    "                 loss_fn(D_fake, torch.zeros_like(D_fake))\n",
    "        optim_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "        gen_imgs = G(z)\n",
    "        D_out = D(gen_imgs)\n",
    "\n",
    "        loss_GAN = loss_fn(D_out, torch.ones_like(D_out))\n",
    "        q_loss = compute_quality(gen_imgs, real_imgs, feature_extractor, alpha, beta)\n",
    "        loss_G = loss_GAN + lambda_q * q_loss\n",
    "\n",
    "        optim_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # === Ghi lại lịch sử\n",
    "    G_losses.append(loss_G.item())\n",
    "    D_losses.append(loss_D.item())\n",
    "    Q_scores.append(q_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: G_loss={loss_G.item():.4f}, D_loss={loss_D.item():.4f}, Q={q_loss.item():.4f}\")\n",
    "    show_and_save_generated_images(G, feature_extractor, epoch, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb299ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TDV\\AppData\\Local\\Temp\\ipykernel_43648\\523632294.py:6: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(filename))\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import glob\n",
    "\n",
    "images = []\n",
    "for filename in sorted(glob.glob(\"gen_images/*.png\")):\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave(\"gan_training.gif\", images, fps=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
