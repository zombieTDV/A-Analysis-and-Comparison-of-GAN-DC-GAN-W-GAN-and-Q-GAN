{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec93c4d",
   "metadata": {},
   "source": [
    "## Tối ưu hàm mục tiêu:\n",
    "$$\n",
    "\\boxed{\n",
    "\\min_{G} E_{z \\sim p_z} [\\mathcal{L}_{\\text{GAN}}(G(z)) + \\lambda \\cdot Q(G(z))]\n",
    "}\n",
    "$$\n",
    "Trong đó:\n",
    "* $G(z)$: ảnh được sinh từ latent vector $z$\n",
    "* $Q(G(z))$: hàm đánh giá **chất lượng ảnh sinh**\n",
    "* $\\lambda$: hệ số điều chỉnh mức phạt của loss chất lượng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f1c9b",
   "metadata": {},
   "source": [
    "### Thay vì chỉ dùng $X$ là độ nhiễu và $Y$ là mức độ thật (mà không rõ đo thế nào), ta định nghĩa lại như sau:\n",
    "\n",
    "**a. Feature distance $D_r$ – Độ khác biệt với ảnh thật:**\n",
    "\n",
    "* Cho ảnh sinh $I_{gen}$, và ảnh thật tương ứng $I_{real}$\n",
    "* Ta tính:\n",
    "$$\n",
    "D_r = \\| \\phi(I_{gen}) - \\phi(I_{real}) \\|_2\n",
    "$$\n",
    "\n",
    "Trong đó $\\phi(\\cdot)$ là feature extractor (VD: tầng giữa của VGG16 hoặc ResNet)\n",
    "\n",
    "$\\to$ **$D_r$ càng nhỏ thì ảnh càng giống thật**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee506e0",
   "metadata": {},
   "source": [
    "**b. Noise-level estimator $N_g$ – Mức độ nhiễu nội tại:**\n",
    "\n",
    "* Dựa trên thống kê gradient hoặc Laplacian:\n",
    "$$\n",
    "N_g = \\text{Var}(\\nabla I_{gen}) \\quad (\\text{hoặc}) \\quad \\text{Laplacian-based energy}\n",
    "$$\n",
    "\n",
    "$\\to$ **Càng nhỏ thì ảnh càng mượt, ít nhiễu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4487d",
   "metadata": {},
   "source": [
    "## Hàm đánh giá chất lượng tổng hợp\n",
    "$$\n",
    "Q(I_{gen}) = \\alpha \\cdot \\text{soft}(N_g) + \\beta \\cdot \\text{soft}(D_r)\n",
    "$$\n",
    "\n",
    "* $\\text{soft}(\\cdot)$ là hàm chuẩn hóa tuyến tính về khoảng [0,1]\n",
    "* $\\alpha, \\beta$: trọng số học được hoặc chọn dựa vào yêu cầu (ví dụ: penalize nhiễu mạnh hơn)\n",
    "\n",
    "$\\to$ **Càng nhỏ, ảnh càng tốt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4805",
   "metadata": {},
   "source": [
    "## Điều kiện đánh giá hoặc regularizer cho GAN\n",
    "**a. Ngưỡng hóa (Thresholding):**\n",
    "$$\n",
    "Q(I_{gen}) < \\tau\n",
    "$$\n",
    "$\\to$ giống như cách bạn đề xuất ban đầu, nhưng với các thành phần có định nghĩa rõ ràng hơn.\n",
    "\n",
    "**b. Dùng làm Regularizer trong loss GAN:**\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{GAN}} + \\lambda \\cdot Q(I_{gen})\n",
    "$$\n",
    "$\\to$ ép Generator sinh ảnh có **nhiễu thấp** và **giống thật** trong feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd0236",
   "metadata": {},
   "source": [
    "## Mô hình tổng quát\n",
    "$$\n",
    "\\boxed{\n",
    "Q(I_{gen}) = \\alpha \\cdot \\text{Norm}(\\text{NoiseLevel}(I_{gen})) + \\beta \\cdot \\text{Norm}(\\| \\phi(I_{gen}) - \\phi(I_{real}) \\|)\n",
    "}\n",
    "$$\n",
    "\n",
    "* **Ngưỡng đánh giá:**\n",
    "    $$\n",
    "    Q(I_{gen}) < \\tau\n",
    "    $$\n",
    "\n",
    "* **Hoặc dùng trong loss:**\n",
    "    $$\n",
    "    \\min_{G} E_{z \\sim p_z} [\\mathcal{L}_{\\text{GAN}}(G(z)) + \\lambda \\cdot Q(G(z))]\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd18f49",
   "metadata": {},
   "source": [
    "**Hàm chuẩn hóa:**\n",
    "$$\n",
    "\\text{Norm}(x) = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n",
    "$$\n",
    "$\\to$ Đưa cả hai thành phần về cùng thang đo [0,1]\n",
    "\n",
    "\n",
    "**Vai trò các hệ số**\n",
    "$$\n",
    "\\alpha + \\beta = 1 \\quad (\\text{nếu cần chuẩn hóa trọng số})\n",
    "$$\n",
    "* $\\alpha$: trọng số cho độ nhiễu $\\to$ kiểm soát độ sắc nét\n",
    "* $\\beta$: trọng số cho độ giống thật $\\to$ kiểm soát tính chân thực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806ca6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "def show_generated_images(generator, extractor, epoch, alpha=0.5, beta=0.5, device='cpu'):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(16, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "        \n",
    "        # Lấy mẫu ảnh thật để so sánh\n",
    "        real_batch = next(iter(dataloader))[0][:16].to(device)\n",
    "        q_score = compute_quality(fake_images, real_batch, extractor, alpha, beta)\n",
    "\n",
    "        grid = vutils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Generated Images - Epoch {epoch+1}\\nQ(I) Score = {q_score.item():.4f}\")\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()\n",
    "    generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12571c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# === Hyperparameters ===\n",
    "latent_dim = 100\n",
    "batch_size = 64\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "lambda_q = 0.1  # đã giảm\n",
    "epochs = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === MNIST Dataset ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# === Generator ===\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z).view(-1, 1, 28, 28)\n",
    "\n",
    "# === Discriminator ===\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Feature Extractor ===\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(*list(D.net.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c170fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hàm tính noise & khoảng cách đặc trưng ===\n",
    "def compute_noise_level(img):\n",
    "    grad_x = img[:, :, :, 1:] - img[:, :, :, :-1]\n",
    "    grad_y = img[:, :, 1:, :] - img[:, :, :-1, :]\n",
    "    return (grad_x.abs().mean() + grad_y.abs().mean())\n",
    "\n",
    "def compute_feature_distance(gen_img, real_img, extractor):\n",
    "    feat_gen = extractor(gen_img)\n",
    "    feat_real = extractor(real_img)\n",
    "    return F.mse_loss(feat_gen, feat_real)\n",
    "\n",
    "# === Chuẩn hóa động Q(I) ===\n",
    "Q_MIN, Q_MAX = 1e10, -1e10\n",
    "\n",
    "def compute_quality(gen_img, real_img, extractor, alpha=0.5, beta=0.5):\n",
    "    global Q_MIN, Q_MAX\n",
    "    noise = compute_noise_level(gen_img)\n",
    "    dist = compute_feature_distance(gen_img, real_img, extractor)\n",
    "    raw_q = alpha * noise + beta * dist\n",
    "\n",
    "    Q_MIN = min(Q_MIN, raw_q.item())\n",
    "    Q_MAX = max(Q_MAX, raw_q.item())\n",
    "\n",
    "    return (raw_q - Q_MIN) / (Q_MAX - Q_MIN + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fd0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_save_generated_images(generator, extractor, epoch, device='cpu'):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(16, latent_dim).to(device)\n",
    "        fake_images = generator(z)\n",
    "\n",
    "        real_batch = next(iter(dataloader))[0][:16].to(device)\n",
    "        q_score = compute_quality(fake_images, real_batch, extractor, alpha, beta)\n",
    "\n",
    "        grid = vutils.make_grid(fake_images, nrow=4, normalize=True)\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Epoch {epoch+1} | Q(I): {q_score.item():.4f}\")\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.savefig(f\"gen_images/epoch_{epoch+1:03}_Q{q_score.item():.2f}.png\")\n",
    "        plt.close()\n",
    "    generator.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56c42eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m fake_imgs = G(z).detach()\n\u001b[32m     23\u001b[39m D_real = D(real_imgs)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m D_fake = \u001b[43mD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m loss_D = loss_fn(D_real, torch.ones_like(D_real)) + \\\n\u001b[32m     27\u001b[39m          loss_fn(D_fake, torch.zeros_like(D_fake))\n\u001b[32m     28\u001b[39m optim_D.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mDiscriminator.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\Study documents\\Python\\A-Analysis-and-Comparison-of-GAN-DC-GAN-W-GAN-and-Q-GAN\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Khởi tạo mô hình ===\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "feature_extractor = FeatureExtractor(D).to(device)\n",
    "\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=1e-3)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=5e-4)  # giảm LR của D\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# === Tạo thư mục lưu ảnh\n",
    "os.makedirs(\"gen_images\", exist_ok=True)\n",
    "\n",
    "G_losses, D_losses, Q_scores = [], [], []\n",
    "\n",
    "# === Huấn luyện ===\n",
    "for epoch in range(epochs):\n",
    "    for real_imgs, _ in dataloader:\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # === Train Discriminator ===\n",
    "        z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "        fake_imgs = G(z).detach()\n",
    "        D_real = D(real_imgs)\n",
    "        D_fake = D(fake_imgs)\n",
    "\n",
    "        loss_D = loss_fn(D_real, torch.ones_like(D_real)) + \\\n",
    "                 loss_fn(D_fake, torch.zeros_like(D_fake))\n",
    "        optim_D.zero_grad()\n",
    "        loss_D.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        # === Train Generator ===\n",
    "        z = torch.randn(real_imgs.size(0), latent_dim).to(device)\n",
    "        gen_imgs = G(z)\n",
    "        D_out = D(gen_imgs)\n",
    "\n",
    "        loss_GAN = loss_fn(D_out, torch.ones_like(D_out))\n",
    "        q_loss = compute_quality(gen_imgs, real_imgs, feature_extractor, alpha, beta)\n",
    "        loss_G = loss_GAN + lambda_q * q_loss\n",
    "\n",
    "        optim_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "    # === Ghi lại lịch sử\n",
    "    G_losses.append(loss_G.item())\n",
    "    D_losses.append(loss_D.item())\n",
    "    Q_scores.append(q_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: G_loss={loss_G.item():.4f}, D_loss={loss_D.item():.4f}, Q={q_loss.item():.4f}\")\n",
    "    show_and_save_generated_images(G, feature_extractor, epoch, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb299ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TDV\\AppData\\Local\\Temp\\ipykernel_43648\\523632294.py:6: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images.append(imageio.imread(filename))\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import glob\n",
    "\n",
    "images = []\n",
    "for filename in sorted(glob.glob(\"gen_images/*.png\")):\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave(\"gan_training.gif\", images, fps=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
